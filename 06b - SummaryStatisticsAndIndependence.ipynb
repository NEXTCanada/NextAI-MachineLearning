{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FI4gjuRVLvEK"
   },
   "source": [
    "*Credit*: some material here has been adapted from [Machine Learning: A Probabilistic Perspective](https://www.cs.ubc.ca/~murphyk/MLbook/) by Kevin P. Murphy (Chapter 2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3dyf5H0Gjwul"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8XsBbx4Aa-Dl"
   },
   "source": [
    "# Summary Statistics and Independence\n",
    "\n",
    "## Expectation\n",
    "\n",
    "The **expected value** of a function $g : \\mathbb{R} \\rightarrow \\mathbb{R}$ of a univariate continuous random variable $X \\sim p(x)$ is defined by:\n",
    "\n",
    "$$\n",
    "  \\mathbb{E}_X \\left[ g(x) \\right] = \\int_{\\mathcal{X}} g(x) p(x) dx .\\\n",
    "$$\n",
    "\n",
    "The expected value of a function $g$ of a discrete random variable $X \\sim p(x)$ is given by:\n",
    "\n",
    "$$\n",
    "  \\mathbb{E}_X \\left[ g(x) \\right] = \\sum_{x \\in \\mathcal{X}} g(x) p(x) .\\\n",
    "$$\n",
    "\n",
    "where $\\mathcal{X}$ is the set of possible outcomes (target space) of the random variable $X$.\n",
    "\n",
    "## Mean and variance\n",
    "\n",
    "The most familiar property of a distribution is its **mean**, or **expected value**, denoted by $\\mu$. For discrete rv's, it is defined as $\\mathbb{E}_X[x] \\triangleq \\sum_{x \\in \\mathcal{X}} x p(x)$, and for continuous rv's, it is defined as $\\mathbb{E}[x] \\triangleq \\int_{\\mathcal{X}} x p(x) dx$.\n",
    "\n",
    "The **variance** is a measure of the \"spread\" of a distribution, denoted by $\\sigma^2$. This is defined as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{V}_X[x] & \\triangleq \\mathbb{E}_X\\left[ \\left( x - \\mu\\right)^2 \\right] = \\int \\left( x - \\mu \\right) ^2 p(x) dx \\\\\\\n",
    "&= \\int x^2 p(x)dx + \\mu^2 \\int p(x) dx - 2 \\mu \\int x p(x) dx = \\mathbb{E}_X[x^2] - \\mu^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "from which we derive the useful result\n",
    "\n",
    "$$\\mathbb{E}_X[x^2] = \\mu^2 + \\sigma^2$$\n",
    "\n",
    "The **standard deviation** is defined as\n",
    "\n",
    "$$\\text{std}[x] \\triangleq \\sqrt{\\mathbb{V}[x]}$$\n",
    "\n",
    "It is often denoted $\\sigma(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Nym2da6AgMi"
   },
   "source": [
    "## Population statistics vs. empirical statistics\n",
    "\n",
    "Let's use `scipy.stats` to explore the difference between population mean and covariance vs. empirical mean and covariance.\n",
    "\n",
    "First we will create a univariate random variable which is normally distributed. We will get into the details of the normal distribution in the next topic. The important thing to know here is that we have access to it's distribution, so we know the true mean and covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IfeRkhvu-TFi"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "rv = multivariate_normal(2.5, 0.5)\n",
    "print(rv.mean)\n",
    "print(rv.cov)\n",
    "\n",
    "x = np.linspace(0, 5, 100, endpoint=False)\n",
    "y = rv.pdf(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.title('Gaussian pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zhVho1sABXfQ"
   },
   "source": [
    "But what if we only had access to samples from the distribution? We would fit the empirical mean and covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m0S6VhXeBd0x"
   },
   "outputs": [],
   "source": [
    "X = rv.rvs(20)  # sample 20 points from the distribution\n",
    "print(X)\n",
    "\n",
    "print(np.mean(X))  # empirical mean\n",
    "print(np.cov(X))   # empirical covariance (=variance in the 1d case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MYajlsniCPK7"
   },
   "source": [
    "Note that our empirical statistics will approach the population statistics as we gain more data. This is a result of the central limit theorem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mO7rjYB8Car_"
   },
   "outputs": [],
   "source": [
    "X = rv.rvs(1000)  # sample 1000 points from the distribution\n",
    "\n",
    "print(np.mean(X))  # empirical mean\n",
    "print(np.cov(X))   # empirical covariance (=variance in the 1d case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZnYp3p_uBJJX"
   },
   "source": [
    "Let's do the same for a bivariate Gaussian, just so we see the difference between the univariate and multivariate case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DKsuF-inCp7W"
   },
   "outputs": [],
   "source": [
    "rv = multivariate_normal([0.5, -0.2], [[2.0, 0.3], [0.3, 0.5]])\n",
    "print(rv.mean)\n",
    "print(rv.cov)\n",
    "\n",
    "x, y = np.mgrid[-1:1:.01, -1:1:.01]\n",
    "pos = np.empty(x.shape + (2,))\n",
    "pos[:, :, 0] = x; pos[:, :, 1] = y\n",
    "\n",
    "plt.contourf(x, y, rv.pdf(pos))\n",
    "plt.title('Gaussian pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z8LYmp2iCp7e"
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Generate samples from the bivariate normal distribution. Estimate the empirical mean and covariance, similar to the univariate case. Experiment with different sample sizes.\n",
    "\n",
    "Make a contour plot which also shows the sampled data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fy1PjTf-6osD"
   },
   "source": [
    "## Independence and conditional independence\n",
    "\n",
    "We say $X$ and $Y$ are **statistically independent**, denoted $X \\perp Y$, if we can represent the joint as the product of the two marginals, i.e.,\n",
    "\n",
    "$$X \\perp Y \\Longleftrightarrow p(x,y) = p(x)p(y)$$\n",
    "\n",
    "<img width=400px src='https://drive.google.com/uc?id=1d5pnEkpZTyK0lesflkha8zkDImMvunpY' />\n",
    "\n",
    "In general, we say a **set** of variables is mutually independent if the joint can be written as a product of marginals.\n",
    "\n",
    "We say $X$ and $Y$ are **conditionally independent** given $Z$ iff the conditional joint can be written as a product of conditional marginals:\n",
    "\n",
    "$$X \\perp Y|Z \\Longleftrightarrow p(x,y|z)=p(x|z)p(y|z)$$\n",
    "\n",
    "\n",
    "In contrast, $X \\perp Y$ would be said to be **unconditionally independent** or **marginally independent** because they are not conditioning on another variable.\n",
    "\n",
    "In machine learning, conditional independence assumptions allow us to build large probabilistic models from small pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GlFgf0KaEYnt"
   },
   "source": [
    "## Covariance and correlation\n",
    "\n",
    "The **covariance** between two rv's $X$ and $Y$ measures the degree to which $X$ and $Y$ are (linearly) related. Covariance is defined as\n",
    "\n",
    "$$\\text{Cov}_{X,Y}[x,y] \\triangleq \\mathbb{E}\\left[\\left(x - \\mathbb{E}_X[X]\\right)\\left(y - \\mathbb{E}_Y[Y]\\right)\\right]=\\mathbb{E}[xy] - \\mathbb{E}[x]\\mathbb{E}[y]$$\n",
    "\n",
    "Note that in the rightmost term above we have left off the subscript denoting the random variable associated with the expectation. This is common practice when the expectation or covariance is clear from its arguments.\n",
    "\n",
    "If we consider a random varible $X$ with states $\\mathbf{x} \\in \\mathbb{R}^D$, its **covariance matrix** is defined to be the following symmetric, positive semi-definite matrix:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{V}_X[\\mathbf{x}] = \\text{Cov}_X[\\mathbf{x},\\mathbf{x}] & \\triangleq \\mathbb{E}_X \\left[\\left(\\mathbf{x} - \\mathbb{E}[\\mathbf{x}]\\right)\\left(\\mathbf{x} - \\mathbb{E}[\\mathbf{x}]\\right)^T\\right]\\\\\n",
    "& = \\left( \\begin{array}{ccc}\n",
    "  \\mathbb{V}[X_1] & \\text{cov}[X_1, X_2] &  \\ldots & \\text{cov}[X_1, X_D] \\\\\n",
    "  \\text{cov}[X_2, X_1] & \\mathbb{V}[X_2] &  \\ldots & \\text{cov}[X_2, X_D] \\\\\n",
    "  \\vdots               & \\vdots          & \\ddots  & \\vdots\\\\\n",
    "  \\text{cov}[X_D, X_1] & \\text{cov}[X_D, X_2] &  \\ldots & \\mathbb{V}[X_D] \n",
    "  \\end{array} \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Covariances can be between $-\\infty$ and $\\infty$. Sometimes it is more convenient to work with a normalized measure, with finite bounds. The (Pearson) **correlation coefficient** between $X$ and $Y$ is defined as\n",
    "\n",
    "$$\\text{corr}[x,y] \\triangleq \\frac{\\text{Cov}[X,Y]}{\\sqrt{\\mathbb{V}[X]\\mathbb{V}[Y]}}$$\n",
    "\n",
    "A **correlation matrix** has the form\n",
    "\n",
    "$$\n",
    "\\text{corr}[\\mathbf{x}, \\mathbf{y}] = \\left( \\begin{array}{ccc}\n",
    "  \\text{corr}[X_1, X_1] & \\text{corr}[X_1, X_2] &  \\ldots & \\text{corr}[X_1, X_d] \\\\\n",
    "  \\text{corr}[X_2, X_1] & \\text{corr}[X_2, X_2] &  \\ldots & \\text{corr}[X_2, X_d] \\\\\n",
    "  \\vdots               & \\vdots          & \\ddots  & \\vdots\\\\\n",
    "  \\text{corr}[X_d, X_1] & \\text{corr}[X_d, X_2] &  \\ldots & \\text{corr}[X_d, X_d] \n",
    "  \\end{array} \\right)\n",
    "$$\n",
    "\n",
    "One can show that $-1 \\leq \\text{corr}[x,y] \\leq 1$. Hence, in a correlation matrix, each entry on the diagonal is 1, and the other entries are between -1 and 1. One can also show that $\\text{corr}[x,y]=1$ iff $Y=aX + b$ for some parameters $a$ and $b$, i.e. there is a *linear* relationship between $X$ and $Y$. A good way to think of the correlation coefficient is as a degree of linearity.\n",
    "\n",
    "If $X$ and $Y$ are independent, meaning $p(\\mathbf{x},\\mathbf{y})=p(\\mathbf{x})p(\\mathbf{y})$, then $\\text{Cov}_{X,Y}[\\mathbf{x},\\mathbf{y}]=\\mathbf{0}$, and hence $\\text{corr}[\\mathbf{x},\\mathbf{y}]=\\mathbf{0}$ so they are uncorrelated. However, the converse is not true: *uncorrelated does not imply independent*. Some striking examples are shown below. The correlation between $X$ and $Y$ is shown at the top of each subplot.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/0/02/Correlation_examples.png\" />\n",
    "\n",
    "Source: https://upload.wikimedia.org/wikipedia/commons/0/02/Correlation_examples.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "untqPC78IwML"
   },
   "source": [
    "### Exercise\n",
    "\n",
    "The *Iris* dataset consists of 3 different types of irises' (Setosa, Versicolour, and Virginica). The iris types are stored as integer classes in the array `target` . Each example has 4 input features: Sepal Length, Sepal Width, Petal Length and Petal Width. These are stored in the array `data`.\n",
    "\n",
    "I've started you off with some code that loads the dataset and partitions the features into an array `X` and labels into an array `y`. For each of the classes, compute the empirical mean and covariance for the whole dataset (you won't need the labels). Then compute the empirical mean and covariance for each class individually.\n",
    "\n",
    "Before you get going, think about what dimensions each of these objects should be.\n",
    "\n",
    "*Iris* is a popular \"toy\" dataset in machine learning and statistics. You can read more information about it on its [Wikipedia page](https://en.wikipedia.org/wiki/Iris_flower_data_set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mAVkZm3jIBxK"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SummaryStatisticsAndIndependence.ipynb",
   "provenance": [
    {
     "file_id": "1gvHh2_ZjzTLMc1CADciI78BD_ezU3Vhd",
     "timestamp": 1568492327806
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
