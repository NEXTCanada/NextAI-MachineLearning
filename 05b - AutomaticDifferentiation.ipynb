{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eFJ26rJK6Sd8"
   },
   "source": [
    "Credit: This is adapted from Roger Grosse and Jimmy Ba's [tutorial from CSC421 at the University of Toronto](https://nbviewer.jupyter.org/url/www.cs.toronto.edu/~rgrosse/courses/csc421_2019/tutorials/tut2/autograd_tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Os4TudAe5MVX"
   },
   "source": [
    "# Automatic Differentiation\n",
    "\n",
    "We are going to use a lightweight automatic differentiation library called Autograd, written by Dougal Maclaurin, David Duvenaud, Matt Johnson, and Jamie Townsend. The popular machine learning libraries TensorFlow and PyTorch have autodiff capabilities, but they have much larger and more complex codebases. Autograd is focused only on autodiff, so its implementation is simple, in fact it doesn't really deviate from the standard NumPy API. That makes it perfect for learning about autodiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VRg6YGJ259JZ"
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np  # Autograd thinly wraps NumPy\n",
    "from autograd import grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nrrnz5Vx68hm"
   },
   "source": [
    "We start by defining a function as usual, just using Python and NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hc4QZwy96MBL"
   },
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    y = np.exp(-x)\n",
    "    return (1.0 - y) / (1.0 + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fvsOIdbk7Bj_"
   },
   "source": [
    "Now we employ `autograd.grad` to create a function that computes the gradient of any function we hand it, in this case `tanh`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dSVg5TWi7B6J"
   },
   "outputs": [],
   "source": [
    "grad_tanh = grad(tanh)\n",
    "\n",
    "# Evaluate the gradient at x = 1.0\n",
    "print(grad_tanh(1.0))\n",
    "\n",
    "# Compare to numeric gradient computed using finite differences\n",
    "print((tanh(1.0001) - tanh(0.9999)) / 0.0002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MlKyVbZh7hBi"
   },
   "source": [
    "## Autograd vs. manual gradients vs. staged computation\n",
    "\n",
    "In the next example, we will see how a complicated computation can be written as the composition of simpler functions. This provides a scalable strategy for computing gradients using the chain rule.\n",
    "\n",
    "Say we wish to write a function to compute the gradient of the sigmoid:\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1 + \\exp(-x)}$$.\n",
    "\n",
    "We can write $\\sigma(x)$ as a composition of several elementary functions: $\\sigma(x) = s(c(b(a(x))))$, where:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a(x) & = -x\\\\\n",
    "b(a) & = \\exp(a)\\\\\n",
    "c(b) & = 1 + b\\\\\n",
    "s(c) & = \\frac{1}{c}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here, we have *staged* the computation such that it contains several intermediate variables, each of which are basic expressions for which we can easily compute the local gradients.\n",
    "\n",
    "The computation graph for this expression is shown below.\n",
    "\n",
    "![computation_graph](https://drive.google.com/uc?id=1asbb2T0o9n4VXKRFuSYoWYU-u7br-vCB)\n",
    "\n",
    "The input to this function is $x$, and the output is represented by node $s$. We aim to compute the gradient of $s$ with respect to $x$, so we can use the chain rule as follows\n",
    "\n",
    "$$\n",
    "\\frac{ds}{dx} = \\frac{ds}{dc}\\frac{dc}{db}\\frac{db}{da}\\frac{da}{dx}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nw9ceGT5VFsV"
   },
   "source": [
    "First, let's see what it would look like if we implemented the chain rule by hand, i.e. no autodiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uXyEL7t28Mzb"
   },
   "outputs": [],
   "source": [
    "def grad_sigmoid_manual(x):\n",
    "    \"\"\"Implements the gradient of the logistic sigmoid function \n",
    "    $\\sigma(x) = 1 / (1 + e^{-x})$ using staged computation\n",
    "    \"\"\"\n",
    "    # Forward pass, keeping track of intermediate values for use in the \n",
    "    # backward pass\n",
    "    a = -x         # -x in denominator\n",
    "    b = np.exp(a)  # e^{-x} in denominator\n",
    "    c = 1 + b      # 1 + e^{-x} in denominator\n",
    "    s = 1.0 / c    # Final result, 1.0 / (1 + e^{-x})\n",
    "    \n",
    "    # Backward pass\n",
    "    dsdc = (-1.0 / (c**2))\n",
    "    dsdb = dsdc * 1\n",
    "    dsda = dsdb * np.exp(a)\n",
    "    dsdx = dsda * (-1)\n",
    "    \n",
    "    return dsdx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y8i2i_rtVYyc"
   },
   "source": [
    "Now let's see the autodiff experience. First we write the forward function. Then we call `autodiff.grad`. That's it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fxRqpMZMVV4J"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    y = 1.0 / (1.0 + np.exp(-x))\n",
    "    return y\n",
    "\n",
    "grad_sigmoid_automatic = grad(sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cGHzgZGIVkhs"
   },
   "source": [
    "Let's compare the two implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Zz7I_ugVoSO"
   },
   "outputs": [],
   "source": [
    "print(grad_sigmoid_automatic(2.0))\n",
    "print(grad_sigmoid_manual(2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kyP2mirTVtRG"
   },
   "source": [
    "We see that automatic differentiation returns the exact result, and it's a lot cleaner in terms of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-2zf-iVkV77j"
   },
   "source": [
    "## Gradients of data structures: `flatten` and `unflatten`\n",
    "\n",
    "Autograd allows you to compute gradients for many different data structures. It provides a lot of flexibility in the types of data structures you can use to store the parameters of a model. This flexibility is achieved through the `autograd.misc.flatten` function, which converts any nested combination of lists, tuples, arrays, or dicts into a 1-dimensional NumPy array.\n",
    "\n",
    "The idea here is that once we know how to compute gradients of vectors, we simply convert many other different data structures into vectors to compute their gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_NkwtI5QXBRz"
   },
   "outputs": [],
   "source": [
    "import autograd.numpy.random as npr\n",
    "from autograd.misc import flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iNaAzXxRXIuI"
   },
   "source": [
    "Let's flatten a list of tuples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ukBHqaEtXQ22"
   },
   "outputs": [],
   "source": [
    "params = [(1.0, 1.0), (2.0, 2.0), (3.0, 3.0, 3.0)]\n",
    "flat_params, unflatten_func = flatten(params)\n",
    "print('Flattened: {}'.format(flat_params))\n",
    "print('Unflattened: {}'.format(unflatten_func(flat_params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SfOEJwQnXYZo"
   },
   "source": [
    "Let's flatten a list of matrices (of different sizes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8yiIzBNkXhzu"
   },
   "outputs": [],
   "source": [
    "params = [npr.randn(3, 3), npr.randn(4, 4), npr.randn(3, 3)]\n",
    "flat_params, unflatten_func = flatten(params)\n",
    "print('Flattened: {}'.format(flat_params))\n",
    "print('Unflattened: {}'.format(unflatten_func(flat_params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8161yaqQXtCX"
   },
   "source": [
    "We may want to represent model parameters in a dictionary, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BzNhhNfzX6Zx"
   },
   "outputs": [],
   "source": [
    "params = {'weights': [1.0, 2.0, 3.0, 4.0], 'biases': [1.0, 2.0]}\n",
    "flat_params, unflatten_func = flatten(params)\n",
    "print('Flattened: {}'.format(flat_params))\n",
    "print('Unflattened: {}'.format(unflatten_func(flat_params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kaibugPTYABW"
   },
   "source": [
    "Or even a dictionary of dictionaries, say, to represent the weights and biases of a neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PGRALak4YLj-"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'layer1': {\n",
    "        'weights': [1.0, 2.0, 3.0, 4.0],\n",
    "        'biases': [1.0, 2.0]\n",
    "    },\n",
    "    'layer2': {\n",
    "        'weights': [5.0, 6.0, 7.0, 8.0],\n",
    "        'biases': [6.0, 7.0]\n",
    "    }\n",
    "}\n",
    "flat_params, unflatten_func = flatten(params)\n",
    "print('Flattened: {}'.format(flat_params))\n",
    "print('Unflattened: {}'.format(unflatten_func(flat_params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F7Cmr_eBYR9H"
   },
   "source": [
    "## Gradient functions\n",
    "\n",
    "Autograd provides several different functions that compute gradients, each with a different signature:\n",
    "\n",
    "*   `grad(fun, argnum=0)` — Returns a function which computes the gradient of `fun` with respect to positional argument number `argnum`. The returned function takes the same arguments as `fun`, but returns the gradient instead. The function `fun` should be scalar-valued. The gradient has the same type as the argument.\n",
    "*   `grad_named(fun, argname)` — Takes gradients with respect to a named argument.\n",
    "*   `multigrad(fun, argnums=[0])` — Takes gradients with respect to multiple arguments simultaneously.\n",
    "*   `multigrad_dict(fun)` — Takes gradients with respect to all arguments simultaneously, and returns a dict mapping `argname` to `gradval`.\n",
    "\n",
    "## Modularity: implementing custom gradients\n",
    "\n",
    "Autograd makes it possible to define custom gradients for your own functions. There are several reasons why you might want to do this, including:\n",
    "\n",
    "1. **Speed**: you may know a faster way to compute the gradient for a specific function.\n",
    "2. **Numerical stability**.\n",
    "3. When your code depends on **external library calls**.\n",
    "\n",
    "The `@primitive` decorator wraps a function so that its gradient can be specified manually and its invocation can be recorded in a computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YXTs8tkRZCkM"
   },
   "outputs": [],
   "source": [
    "from autograd.extend import primitive, defvjp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S-y2zOjJaUSY"
   },
   "source": [
    "First we write our function and decorate it with `@primitive`. This tells autograd not to look inside this function, but instead to treat it as a black box, whose gradient might be specified later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sJmCfCkKaoes"
   },
   "outputs": [],
   "source": [
    "@primitive\n",
    "def logsumexp(x):\n",
    "    \"\"\"Numerically stable log(sum(exp(x)))\"\"\"\n",
    "    max_x = np.max(x)\n",
    "    return max_x + np.log(np.sum(np.exp(x - max_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kILDJ6v0ar39"
   },
   "source": [
    "Next, we write a function that specifies the gradient with a closure (a function defined within another function, with access to the parent function's variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zuJwFyD6ax7d"
   },
   "outputs": [],
   "source": [
    "def make_grad_logsumexp(ans, x):\n",
    "    # If you want to be able to take higher-order derivatives, then all the\n",
    "    # code inside this function must be itself differentiable by autograd.\n",
    "    def gradient_product(g):\n",
    "        return np.full(x.shape, g) * np.exp(x - np.full(x.shape, ans))\n",
    "    return gradient_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yLOAdBC4bBLV"
   },
   "source": [
    "Then we tell Autograd that logsumexp has a gradient-making function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BZfWHD9DbIyF"
   },
   "outputs": [],
   "source": [
    "defvjp(logsumexp, make_grad_logsumexp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sb5s_RKZbtRb"
   },
   "source": [
    "That's it! Now we can use `logsumexp` inside a larger function that we wish to differentiate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zlokk9_Rb0S0"
   },
   "outputs": [],
   "source": [
    "# Now we can use logsumexp() inside a larger function that we want to differentiate.\n",
    "def example_func(y):\n",
    "    z = y**2\n",
    "    lse = logsumexp(z)\n",
    "    return np.sum(lse)\n",
    "\n",
    "grad_of_example = grad(example_func)\n",
    "print(\"Gradient: \", grad_of_example(npr.randn(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YEqABm66b3em"
   },
   "source": [
    "Autograd provides its own utility for checking gradients numerically. This will fail if a mismatch occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ydQBCkSdcITF"
   },
   "outputs": [],
   "source": [
    "from autograd.test_util import check_grads\n",
    "check_grads(example_func, modes=['rev'], order=2)(npr.randn(10))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AutomaticDifferentiation.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
