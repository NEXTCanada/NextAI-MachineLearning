{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FI4gjuRVLvEK"
   },
   "source": [
    "*Credit*: some material here has been adapted from [Sam Roweis](https://www.cs.nyu.edu/home/people/in_memoriam/samroweis.html)' [Linear Algebra Review](http://www.cs.ubc.ca/~murphyk/Teaching/Papers/roweis_linAlgebra.ps).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3dyf5H0Gjwul"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6DqKeu9tLvEO"
   },
   "source": [
    "# Vectors and Matrices\n",
    "\n",
    "Linear algebra is the study of vectors and matrices and how they can be manipulated to perform various calculations. \n",
    "\n",
    "We will start by exploring some simple operations on vectors and matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DTtj9PKLkKD0"
   },
   "source": [
    "## Addition and scaling\n",
    "\n",
    "Adding up two vectors or two matrices is easy: just add their corresponding elements (of course the two must be the same shape!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N713kokaLvEO"
   },
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "y = np.ones(3, )\n",
    "print(x)\n",
    "print(y)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vTE2g5Jckb24"
   },
   "source": [
    "Note that when printing vectors, NumPy will format them as row vectors. But to stay consistent with [Mathematics of Machine Learning](https://mml-book.github.io/), for vectors where we are not explicit about whether they are a row- or column-vector, that is $\\mathbf{x} \\in \\mathbb{R}^n$ as opposed to $\\mathbf{x} \\in \\mathbb{R}^{1 \\times n}$ or $\\mathbf{x} \\in \\mathbb{R}^{n \\times 1}$, we will call these column vectors.\n",
    "\n",
    "Do you know why there is a decimal point following the elements of the second and third vectors above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jEkNbhK-LvES"
   },
   "outputs": [],
   "source": [
    "A = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "B = np.arange(6).reshape(A.shape)\n",
    "print(A)\n",
    "print(B)\n",
    "print(A + B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LuQ3sRGqLvEU"
   },
   "source": [
    "Multiplying a vector or matrix by a scalar just multiplies each element by the scalar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "amYDCZDYLvEV"
   },
   "outputs": [],
   "source": [
    "print(2 * x)\n",
    "print(0.5 * A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYI8WT_0LvEX"
   },
   "source": [
    "## Matrix-vector and matrix-matrix multiplication\n",
    "\n",
    "A good way to think of an $n \\times m$ matrix $A$ is as a machine that eats $m$ sized vectors and spits out $n$ sized vectors. This conversion process is known as (left) multiplying by $A$ and has many similarities to scalar multiplication, but also a few differences. Most importantly, the machine only accepts inputs of the right size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3DHG6pm5LvEY"
   },
   "outputs": [],
   "source": [
    "# In Numpy, both matrix-vector and matrix-matrix multiplication is performed by np.dot\n",
    "print(A.shape)\n",
    "print(y.shape)\n",
    "print(np.dot(A, x))\n",
    "print(A.dot(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BG9Wi-9iLvEa"
   },
   "outputs": [],
   "source": [
    "print(np.dot(A, x[:2]))  # not compatible sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kk-Ph7mqLvEd"
   },
   "source": [
    "Like scalar multiplication, matrix multiplication is *distributive* and *associative*:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A(\\mathbf{x} + \\mathbf{y}) & = A\\mathbf{x} + A\\mathbf{y}\\\\\n",
    " B(A\\mathbf{x}) & = (BA)\\mathbf{x}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "One way to think of this is that the matrix product $BA$ is the equivalent linear operator you get if you compose the action of $A$ followed by the action of $B$.\n",
    "\n",
    "Matrix-matrix multiplication can be thought of as a sequence of matrix-vector multiplications, one for each column, whose results get stacked beside each other in columns to form a new matrix. In general, we can think of column vectors of length $n$ as just $n \\times 1$ and row vectors as $1 \\times n$ matrices. This eliminates any distinction between matrix-matrix and matrix-vector multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Nv7cOuhLvEd"
   },
   "outputs": [],
   "source": [
    "print(np.dot(A, A.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t4OF38bFLvEg"
   },
   "source": [
    "Note that in the above, we flipped or \"transposed\" the matrix. This interchanges the rows and columns, and in the example above, made the shapes compatible for matrix-matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NdsN836ALvEg"
   },
   "outputs": [],
   "source": [
    "print(A.shape)\n",
    "print(A.T.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0c0QDpxhLvEj"
   },
   "source": [
    "Unlike scalar multiplication, matrix multiplication is not *commutative*:\n",
    "\n",
    "$$ A \\mathbf{x} \\neq \\mathbf{x} A $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bgnTNRyfnq4D"
   },
   "source": [
    "## Inverses\n",
    "\n",
    "First, let's consider the concept of reversing or undoing or *inverting* the function represented by a matrix $A$. For a function to be invertible, there needs to be a one-to-one relationship between inputs and outputs so that given the output you can always say exactly what the input was. In other words, we need a function which, when composed with $A$ gives back the original vector. Such a function -- if it exists -- is called the *inverse* of $A$ and the matrix is denoted $A^{-1}$.\n",
    "\n",
    "In matrix terms, we seek a matrix that left multiplies $A$ to give the identity matrix:\n",
    "\n",
    "$$A^{-1}A = I$$\n",
    "\n",
    "The identity matrix, $I_{ij} = \\delta_{ij}$ corresponds to the identity (do-nothing) function.\n",
    "\n",
    "Only a few, special linear functions are invertible. \n",
    "\n",
    "* They must have at least as many outputs as inputs\n",
    "* They must not map any two inputs to the same output\n",
    "\n",
    "Technically this means that they must have *full rank*, a concept which we will get to later.\n",
    "\n",
    "Non-square matrices ($m$-by-$n$ matrices for which $m \\neq n$) technically do not have an inverse. However, in some cases such a matrix may have a *left inverse* or *right inverse* (but not both). If A is $m$-by-$n$ and the rank of $A$ is equal to $n$, then $A$ has a left inverse: an $n$-by-$m$ matrix $B$ such that $BA = I$. If $A$ has rank $m$, then it has a right inverse: an $n$-by-$m$ matrix $B$ such that $AB = I$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tfD-ks8jLvEp"
   },
   "outputs": [],
   "source": [
    "C = A.dot(A.T)  # A trick to make an invertible matrix\n",
    "C_inv = np.linalg.inv(C)\n",
    "print(C)\n",
    "print(C_inv) \n",
    "print(C_inv.dot(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oGfAfszkoyq3"
   },
   "source": [
    "Why are the entries of the result not exactly zero?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fVhlqTmMp80Q"
   },
   "source": [
    "## Einsum\n",
    "\n",
    "[Mathematics of Machine Learning](https://mml-book.github.io/) gives an example of using the ``np.einsum`` function. Einsum (short for Einstein summation) is implemented in NumPy, as well as deep learning libraries such as TensorFlow and PyTorch.\n",
    "\n",
    "Einsum is a little-known but extremely general function, which is a compact and elegant way to specify almost any product of scalars, vectors, matrices and their higher-order generalizations, tensors.\n",
    "\n",
    "Let's walk through the example given in the book (we also formulate matrices $A$ and $B$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qudiQ4W5qFeh"
   },
   "outputs": [],
   "source": [
    "A = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "B = np.arange(6).reshape(A.T.shape)\n",
    "C = np.einsum('il,lj', A, B)\n",
    "print(A)\n",
    "print(B)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IbChCqvSqPAx"
   },
   "source": [
    "This is short form for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d_Vdt7BtqPsg"
   },
   "outputs": [],
   "source": [
    "C = np.einsum('il,lj->ij', A, B)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XUOJYjoUsLSc"
   },
   "source": [
    "Einsum is invoked with a format string (the first argument), and any number of NumPy tensors as the following arguments. It returns a result tensor.\n",
    "\n",
    "The format string contains commas which separate the specifications of the arguments, as well as an arrow ``->`` which separates the specifications of the input arguments from the specifications of the result tensor.\n",
    "\n",
    "The specifications of the input arguments and result tensors are a series of alphabetical ASCII characters. The number of characters in an argument's specification is exactly equal to its dimensions.\n",
    "\n",
    "So here, ``il`` refers to the dimensions of matrix $A$, ``lj`` refers to the dimensions of matrix $B$ (note that they are compatible for multiplication) and ``ij`` refers to the dimensions of the output matrix $C$.\n",
    "\n",
    "This particular format string defines a matrix multiplication between $A$ and $B$ to produce $C$.\n",
    "\n",
    "We can do much more complicated expressions, like batched outer products, like so:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hA9G8E23vWbB"
   },
   "outputs": [],
   "source": [
    "P = np.random.rand(4, 2)\n",
    "Q = np.random.rand(4, 3)\n",
    "R = np.einsum('bi,bj->bij', P, Q)\n",
    "print(P)\n",
    "print(Q)\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d-Y_VtI4wbOJ"
   },
   "source": [
    "This Einsum computes the outper product of the $b$th row of $P$ and the $b$th row of $Q$ for all $b$. This is difficult (and bug-prone) to do with other functions.\n",
    "\n",
    "### Further reading\n",
    "\n",
    "For a basic introduction to NumPy's Einsum and a list of simple operations achieved by it, see [Alex Riley's blog](http://ajcr.net/Basic-guide-to-einsum/). For a much more detailed introduction to Einsum and a discussion of its inner workings, see [Olexa Bilaniuk's blog](https://obilaniu6266h16.wordpress.com/2016/02/04/einstein-summation-in-numpy/).  For a more advanced primer of its use in TensorFlow and PyTorch, see [Tim Rocktäschel's blog](https://rockt.github.io/2018/04/30/einsum)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "VectorsAndMatrices.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
