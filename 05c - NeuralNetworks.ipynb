{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribution: \n",
    "\n",
    "   * Most material is adapted from [Justin Johnson's PyTorch Examples](https://github.com/jcjohnson/pytorch-examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Networks\n",
    "\n",
    "## Warm-up: numpy\n",
    "\n",
    "We are going to first build a simple neural network in numpy, and then we will re-build it in PyTorch observing the convenience working under that framework.\n",
    "\n",
    "Numpy provides an n-dimensional array object, and many functions for manipulating these arrays. Numpy is a generic framework for scientific computing; it does not know anything about computation graphs, or deep learning, or gradients. However we can easily use numpy to fit a two-layer network to random data by manually implementing the forward and backward passes through the network using numpy operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "  # Forward pass: compute predicted y\n",
    "  h = x.dot(w1)\n",
    "  h_relu = np.maximum(h, 0)\n",
    "  y_pred = h_relu.dot(w2)\n",
    "  \n",
    "  # Compute and print loss\n",
    "  loss = np.square(y_pred - y).sum()\n",
    "  print(t, loss)\n",
    "  \n",
    "  # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "  grad_y_pred = 2.0 * (y_pred - y)\n",
    "  grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "  grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "  grad_h = grad_h_relu.copy()\n",
    "  grad_h[h < 0] = 0\n",
    "  grad_w1 = x.T.dot(grad_h)\n",
    " \n",
    "  # Update weights\n",
    "  w1 -= learning_rate * grad_w1\n",
    "  w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few issues with the numpy implementation of our network.\n",
    "\n",
    "First of all, numpy cannot utilize GPUs to accelerate its numerical computations. For modern deep neural networks, GPUs often provide speedups of [50x or greater](https://github.com/jcjohnson/cnn-benchmarks), so pure numpy is not really enough for large-scale learning.\n",
    "\n",
    "Moreover, we had to compute and implement the gradients by hand. This is time-consuming and error-prone. For example, if we wanted to make a change to the loss function, or use a different kind of nonlinearity at the hiddens, we need to update and test both our forward and backward pass.\n",
    "\n",
    "Let's see how painful that can be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. This network uses a squared error function. Change this to use an exponential cost of the form:\n",
    "\n",
    "   $ \\sum_n \\exp \\left( \\sum_j \\left| y_j^{\\text{pred}} - y_j \\right| \\right)$\n",
    "\n",
    "   where $n$ indexes examples and $j$ indexes output dimensions.\n",
    "\n",
    "2. Now change your hidden layer such that it uses sigmoid units, $a(x_i) = 1/ \\left( 1 + \\exp(-x_i) \\right)$ instead of RELU units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Tensors\n",
    "\n",
    "PyTorch Tensors can utilize GPUs to accelerate their numeric computations. To run a PyTorch Tensor on a GPU, you simply need to cast it to a new datatype.\n",
    "\n",
    "We will now re-write our example above to use PyTorch Tensors. Let's first pretend we don't know anything about Variables and automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.Tensor\n",
    "# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in).type(dtype)\n",
    "y = torch.randn(N, D_out).type(dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H).type(dtype)\n",
    "w2 = torch.randn(H, D_out).type(dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "  # Forward pass: compute predicted y\n",
    "  h = x.mm(w1)\n",
    "  h_relu = h.clamp(min=0)\n",
    "  y_pred = h_relu.mm(w2)\n",
    "\n",
    "  # Compute and print loss\n",
    "  loss = (y_pred - y).pow(2).sum()\n",
    "  print(t, loss)\n",
    "\n",
    "  # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "  grad_y_pred = 2.0 * (y_pred - y)\n",
    "  grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "  grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "  grad_h = grad_h_relu.clone()\n",
    "  grad_h[h < 0] = 0\n",
    "  grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "  # Update weights using gradient descent\n",
    "  w1 -= learning_rate * grad_w1\n",
    "  w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what we've gained here is the ability to run our computations on either CPU or GPU, but we haven't gained any flexibility or time savings from the perspective of implementing our neural net.\n",
    "\n",
    "In the above examples, we had to manually implement both the forward and backward passes of our neural network. In fact, this really isn't a big deal for the small two-layer network, but can quickly get hairy for large complex networks.\n",
    "\n",
    "By using use PyTorch Variables instead of Tensors to implement our two-layer network, we no longer need to manually implement the backward pass through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exercises\n",
    "\n",
    "1. Re-write the above Tensor-based example to use Variables and autograd:\n",
    "\n",
    "  - You will need to do `from torch.autograd import Variable`\n",
    "  - You will need to replace all Tensor creation by Variable creation\n",
    "  - You will need to replace all the lines of gradient computation by a single call to `loss.backward()`\n",
    "  - You can then depend directly on the gradient attributes of your Variables, e.g. `w1.grad.data` and `w1.grad.data`\n",
    "  - You should manually zero out your gradients after performing a weight update (otherwise they will be accumulated)\n",
    "  - Make sure you don't mix Tensors and Variables in computation\n",
    "  \n",
    "2. If you haven't yet done so, refactor your code to eliminate intermediate values: `h`, `h_relu`. Note that we no longer need to keep references to intermediate values when we are not implementing the backward pass by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further reading:**\n",
    "\n",
    "You can read about [Defining your own autograd functions](http://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-defining-new-autograd-functions)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
