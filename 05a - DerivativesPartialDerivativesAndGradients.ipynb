{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jQMurOMNcRKy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ct9uDAsFcSsI"
   },
   "source": [
    "# Derivatives, Partial Derivatives, and Gradients\n",
    "\n",
    "We are making our way towards automatic differentiation or \"autodiff\". But before we get there, let's explore some related but distinct concepts.\n",
    "\n",
    "## Finite differences\n",
    "\n",
    "Derivatives can be approximated numerically using a method of numerical differentiation known as finite differences. The one-sided definition looks much like the difference quotient:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(\\mathbf{x})}{\\partial x_i} = \\frac{\\partial f(x_1, x_2, \\ldots, x_N)}{x_i} = \\frac{f(x_1, \\ldots, x_i + h, \\ldots, x_N) - f(x_1, \\ldots, x_i, \\ldots, x_N) }{h}\n",
    "$$\n",
    "\n",
    "where we have perturbed the input of interest by a tiny amount, $h$.\n",
    "\n",
    "There is a more numerically stable two-sided version as well:\n",
    "\n",
    "$$\n",
    " \\frac{\\partial f(x_1, x_2, \\ldots, x_N)}{\\partial x_i} = \\frac{f(x_1, \\ldots, x_i + h, \\ldots, x_N) - f(x_1, \\ldots, x_i - h, \\ldots, x_N) }{2h}\n",
    "$$.\n",
    "\n",
    "Let's apply the method of finite differences to the softplus function, which shows up in neural networks. It has a convenient analytical derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H6sfMx4qg8cB"
   },
   "outputs": [],
   "source": [
    "# softplus function\n",
    "def f(x):\n",
    "  return np.log(1 + np.exp(x))\n",
    "\n",
    "# derivative of softplus is the sigmoid\n",
    "def fprime(x):\n",
    "  return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sVC4Cu47r3ZH"
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Show (on paper) that the derivative of the softplus is the sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "56UuiPQGcnjd"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 20)\n",
    "y = f(x) \n",
    "plt.plot(x, y)\n",
    "plt.title('Softplus')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eBvhe2L6sFal"
   },
   "source": [
    "Let's use an off-the-shelf implementation of finite differences. We can adjust the method ('central', 'forward', 'backward') and the amount of perturbation, $h$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "82DUO7vxfSqf"
   },
   "outputs": [],
   "source": [
    "#Source: https://www.math.ubc.ca/~pwalls/math-python/differentiation/\n",
    "def derivative(f, a, method='central', h=0.01):\n",
    "    '''Compute the difference formula for f'(a) with step size h.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : function\n",
    "        Vectorized function of one variable\n",
    "    a : number\n",
    "        Compute derivative at x = a\n",
    "    method : string\n",
    "        Difference formula: 'forward', 'backward' or 'central'\n",
    "    h : number\n",
    "        Step size in difference formula\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Difference formula:\n",
    "            central: f(a+h) - f(a-h))/2h\n",
    "            forward: f(a+h) - f(a))/h\n",
    "            backward: f(a) - f(a-h))/h            \n",
    "    '''\n",
    "    if method == 'central':\n",
    "        return (f(a + h) - f(a - h)) / (2 * h)\n",
    "    elif method == 'forward':\n",
    "        return (f(a + h) - f(a)) / h\n",
    "    elif method == 'backward':\n",
    "        return (f(a) - f(a - h)) / h\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'central', 'forward' or 'backward'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LBYV18MdslB-"
   },
   "source": [
    "We see that finite differences, even using the default step size, is pretty accurate. Note that when checking our analytical gradients by hand in practice, typically we use a much smaller stepsize, e.g. $1 \\times 10^{-8}$ or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5JJWJ7T9g0gc"
   },
   "outputs": [],
   "source": [
    "print(fprime(0))\n",
    "print(derivative(f, 0, method='forward'))\n",
    "print(derivative(f, 0, method='central'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WVGG-d_6s40-"
   },
   "source": [
    "Let's plot the true derivative, and the finite difference estimates at several points along the domain of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jtNASDCphFgn"
   },
   "outputs": [],
   "source": [
    "plt.plot(x, y, label='softplus')\n",
    "plt.plot(x, fprime(x), label='true derivative')\n",
    "plt.plot(x, derivative(f, x), 'g.', label='central difference')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RMHP_yNRh5q9"
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Write a replacement function for `derivative` that works on functions of multiple variables. Plot the central difference estimate for a function of two variables, for each partial derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S-UfQHuDl0Li"
   },
   "source": [
    "## Symbolic Differentiation\n",
    "\n",
    "Symbolic differentiation libraries can take a mathematical expression for a function and return a mathematical expression for the derivative. It is convenient for simple expression but can become unwieldy for more complex expressions.\n",
    "\n",
    "The SymPy library provides symbolic differentiation functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FhXPDnEooXvw"
   },
   "outputs": [],
   "source": [
    "import sympy\n",
    "from sympy import init_printing\n",
    "\n",
    "# Get pretty printing to work correctly in Colab: https://stackoverflow.com/a/52959734\n",
    "def custom_latex_printer(exp,**options):\n",
    "    from google.colab.output._publish import javascript\n",
    "    url = \"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=default\"\n",
    "    javascript(url=url)\n",
    "    return sympy.printing.latex(exp,**options)\n",
    "init_printing(use_latex=\"mathjax\",latex_printer=custom_latex_printer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ONaIYCkmeh9"
   },
   "outputs": [],
   "source": [
    "x = sympy.symbols('x')\n",
    "f = sympy.log(1 + sympy.exp(x))\n",
    "sympy.diff(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ssebKci9mk_W"
   },
   "source": [
    "Notice that SymPy didn't simplify the result into the more common expression for the sigmoid $1/\\left(1 + \\exp(-x)\\right)$. That's one of the downsides of this technique.\n",
    "\n",
    "SymPy can take derivatives of functions of multiple variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4gQ83-zZn7Gl"
   },
   "outputs": [],
   "source": [
    "x1, x2 = sympy.symbols('x1, x2')\n",
    "f = x1**3 + 2 * x2**2\n",
    "display(sympy.diff(f, x1))\n",
    "display(sympy.diff(f, x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "esMbrOd0pHUj"
   },
   "source": [
    "Sympy can also take derivatives with respect to many variables at once. Just pass each derivative in order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pxVQJtPdpJKC"
   },
   "outputs": [],
   "source": [
    "display(sympy.diff(f, x1, x1))  # d^2f/dx1^2\n",
    "display(sympy.diff(f, x1, x2))  # d^2f/(dx1 dx2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DnX59iIYpxSJ"
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Use SymPy to evaluate $\\frac{df}{dt}$ for $f(x_1, x_2) = x_1^2 + 2x_2$, where $x_1 = \\sin t$ and $x_2 = \\cos t$. Check your answer with Example 5.8 in [Mathematics for Machine Learning](https://mml-book.github.io/)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DerivativesPartialDerivativesAndGradients.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
