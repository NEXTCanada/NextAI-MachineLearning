{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4pJysFMoQ2Du"
   },
   "source": [
    "*Credit*: These examples have been adapted from [Scipy Tutorial](https://docs.scipy.org/doc/scipy-0.18.1/reference/tutorial/optimize.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C135LrgaNh2P"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "STzZKSE7agFL"
   },
   "source": [
    "# Unconstrained Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12l1n44ZNh2S"
   },
   "source": [
    "The `scipy.optimize` package provides several commonly used optimization algorithms. A detailed listing can be found by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fUHlKAcDNh2T"
   },
   "outputs": [],
   "source": [
    "help(scipy.optimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i_zldt8cNh2W"
   },
   "source": [
    "For Machine Learning, we are mainly interested in unconstrained minimization of multivariate scalar functions (typically where gradient information is available). In addition to several algorithms for unconstrained minimization of multivariate scalar functions (e.g. BFGS, Nelder-Mead simplex, Newton Conjugate Gradient, etc.) the module also contains:\n",
    "- Global (brute-force) optimization routines\n",
    "- Least-squares minimization (which we saw before in the Solving Systems of Linear Equations notebook)\n",
    "- Scalar univariate function minimizers and root finders; and\n",
    "- Multivariate equation system solvers using a variety of algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZCRu1Lo3Nh2X"
   },
   "source": [
    "# Unconstrained minimization of multivariate scalar functions (`minimize`)\n",
    "\n",
    "The `minimize` function provides a common interface to unconstrained and constrained minimization algorithms for multivariate scalar functions. To demonstrate the minimization function, let's consider the problem of minimizing the [Rosenbrock function](https://en.wikipedia.org/wiki/Rosenbrock_function) of $N$ variables:\n",
    "$$ f\\left(\\mathbf{x}\\right)=\\sum_{i=1}^{N-1}100\\left(x_{i}-x_{i-1}^{2}\\right)^{2}+\\left(1-x_{i-1}\\right)^{2}.$$\n",
    "\n",
    "The minimum value of this function is 0 which is achieved when $x_i=1$.\n",
    "\n",
    "Note that the Rosenbrock function and its derivatives are included in `scipy.optimize`. The implementations in the following provide examples of how to define an objective function as well as its Jacobian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v61sEi1GNh2X"
   },
   "source": [
    "## Nelder-Mead Simplex algorithm (`method='Nelder-Mead'`)\n",
    "\n",
    "In the example below, the `minimize` routine is used with the *Nelder-Mead* simplex algorithm (selected through the `method` parameter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AFHfq7AxNh2Y"
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qS78kd2ZNh2a"
   },
   "outputs": [],
   "source": [
    "def rosen(x):\n",
    "    \"\"\"The Rosenbrock function\"\"\"\n",
    "    return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zqcgpetANh2c"
   },
   "outputs": [],
   "source": [
    "x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n",
    "res = minimize(rosen, x0, method='nelder-mead',\n",
    "               options={'xtol': 1e-8, 'disp': True})\n",
    "print(res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pw7bDTYxNh2e"
   },
   "source": [
    "The simplex method is a simple way to minimize a fairly well-behaved function. It only requires function evaluations and is a good choice for simple minimization problems. However, because it does not use any gradient evaluations, it may take longer to find the minimum compared to a gradient-based method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2p9sUTdXSrqh"
   },
   "source": [
    "## Gradient descent (by hand)\n",
    "\n",
    "Before we move on to one of the `scipy.optimize` gradient-based methods, let's implement basic gradient descent by hand. Normally you shouldn't really do this yourself. Instead, you should rely on a properly debugged and optimized implementation of an optimizer. But this is a learning exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kr011HkXT4dj"
   },
   "source": [
    "To demonstrate gradient descent, the Rosenbrock function is used again. The gradient of the Rosenbrock function is the vector:\n",
    "\n",
    "$$ \\begin{eqnarray*} \\frac{\\partial f}{\\partial x_{j}} & = & \\sum_{i=1}^{N}200\\left(x_{i}-x_{i-1}^{2}\\right)\\left(\\delta_{i,j}-2x_{i-1}\\delta_{i-1,j}\\right)-2\\left(1-x_{i-1}\\right)\\delta_{i-1,j}.\\\\  & = & 200\\left(x_{j}-x_{j-1}^{2}\\right)-400x_{j}\\left(x_{j+1}-x_{j}^{2}\\right)-2\\left(1-x_{j}\\right).\\end{eqnarray*}$$\n",
    "\n",
    "This expression is valid for the interior derivatives. Special cases are:\n",
    "\n",
    "$$ \\begin{eqnarray*} \\frac{\\partial f}{\\partial x_{0}} & = & -400x_{0}\\left(x_{1}-x_{0}^{2}\\right)-2\\left(1-x_{0}\\right),\\\\ \\frac{\\partial f}{\\partial x_{N-1}} & = & 200\\left(x_{N-1}-x_{N-2}^{2}\\right).\\end{eqnarray*} $$\n",
    "\n",
    "A function which computes this gradient is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWSaO1btTpzN"
   },
   "outputs": [],
   "source": [
    "# note the special handling of the exterior derivatives\n",
    "def rosen_der(x):\n",
    "    xm = x[1:-1]\n",
    "    xm_m1 = x[:-2]\n",
    "    xm_p1 = x[2:]\n",
    "    der = np.zeros_like(x)\n",
    "    der[1:-1] = 200*(xm-xm_m1**2) - 400*(xm_p1 - xm**2)*xm - 2*(1-xm)\n",
    "    der[0] = -400*x[0]*(x[1]-x[0]**2) - 2*(1-x[0])\n",
    "    der[-1] = 200*(x[-1]-x[-2]**2)\n",
    "    return der"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1uyEJh12UHlw"
   },
   "outputs": [],
   "source": [
    "x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n",
    "num_iter = 1000  # number of gradient descent updates\n",
    "step_size = 1e-4\n",
    "x = x0.copy()\n",
    "for i in range(num_iter):\n",
    "  x -= step_size * rosen_der(x)\n",
    "  if (i + 1) % 10 == 0:\n",
    "    print(f'Iteration {i + 1}, feval {rosen(x)}')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8rvNHGo1XdDr"
   },
   "source": [
    "We see that basic gradient descent doesn't seem to converge any quicker than the Nelder-Mead method. But there are still a couple of tricks up our sleeve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "et6xnsGdUZM2"
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Here we used a constant step size. Implement a version of gradient descent which decays the step size at each iteration. Experiment with different schedules. You can find examples of a few different kinds of schedule in [Stanford's cs231n course notes](http://cs231n.github.io/neural-networks-3/#annealing-the-learning-rate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rji76G9FXOD-"
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Try implementing gradient descent with momentum, as described in $\\S$ 7.1.2 of [Mathematics for Machine Learning](https://mml-book.github.io/). Does this improve convergence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0MJMmvDeYNJJ"
   },
   "source": [
    "### Exercise\n",
    "\n",
    "We ran our optimizer for a fixed number of iterations. But Scipy's optimizers test for convergence. Can you write a test for convergence and terminate gradient descent automatically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ikbIlog6Nh2e"
   },
   "source": [
    "## Broyden-Fletcher-Golfarb-Shanno algorithm (`method='BFGS'`)\n",
    "\n",
    "Let's turn now to a more \"industrial-strength\" gradient descent-based optimizer. In contrast to Nelder-Mead, this routine uses the gradient of the objective function which should enable it to converge more quickly to a solution. If the gradient is not given by the user, then it is estimated using first-differences. The Broyden-Fletcher-Golfarb-Shanno (BFGS) method typically requires fewer calls than the simplex algorithm even when the gradient must be estimated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VbA8CrdiNh2h"
   },
   "source": [
    "We will reuse our definition of `rosen_dir` from above. This gradient information is specified in the `minimize` function through the jac parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9RZdEQqgNh2h"
   },
   "outputs": [],
   "source": [
    "res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n",
    "               options={'disp': True})\n",
    "print(res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kw_r0lT2Nh2j"
   },
   "source": [
    "Machine learning libraries (e.g. TensorFlow, PyTorch etc.) will provide a similar interface. When they provide auto-differentiation capabilities, you will not need to worry about writing the derivative function yourself. You will need to provide the \"forward\" computational graph and an objective function."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "UnconstrainedOptimization.ipynb",
   "provenance": [
    {
     "file_id": "1u9v8Xc7KilebO7Wti1ysIJWQmWNi8VS2",
     "timestamp": 1569981925535
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
