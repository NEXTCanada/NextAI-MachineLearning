{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4pJysFMoQ2Du"
   },
   "source": [
    "*Credit*: These examples originally came from [The Kitchin Research Group Blog](http://kitchingroup.cheme.cmu.edu/blog/2018/11/03/Constrained-optimization-with-Lagrange-multipliers-and-autograd/). I have adapted them to be more verbose, use notation consistent with [Mathematics for Machine Learning](https://mml-book.github.io/) and add some colour. I've also repeated some of the documentation pertaining to the `minimize` module [from the SciPy docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "STzZKSE7agFL"
   },
   "source": [
    "# Constrained Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12l1n44ZNh2S"
   },
   "source": [
    "In the previous notebook we saw that the `scipy.optimize` package provides several unconstrained optimization algorithms. It is rarer to need constrained optimization algorithms in machine learning, but if we need them, they are available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RpJ9QOxb2t9f"
   },
   "source": [
    "## Method of Lagrange multipliers\n",
    "\n",
    "Let's first use Python to help us implement the method of Lagrange multipliers. Consider the following constrained optimization problem in three variables:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min_{x_1, x_2, x_3} f(\\mathbf{x}) = x_1^2 + x_2^2 + x_3^2\\\\\n",
    "\\text{subject to}:   h(\\mathbf{x}) = 2x_1 - x_2 + x_3 - 3 = 0 .\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Geometrically this corresponds to finding a point on a plane $2x_1 - x_2 + x_3 = 3$ which is closest to the origin $\\mathbf{x} = \\mathbf{0}$.\n",
    "\n",
    "\n",
    "We can convert this into an unconstrained problem by using the method of Lagrange multiplers. The Lagrangian is\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}) = f(\\mathbf{x}) + \\lambda h(\\mathbf{x}) .\\\n",
    "$$\n",
    "\n",
    "Note that because $h(\\cdot)$ is an equality rather than an inequality constraint, we leave $\\lambda$ unconstrained rather than restricting it to be non-negative.\n",
    "\n",
    "We can take the gradient of the Lagrangian with respect to $\\mathbf{x}$ and setting the differential to zero, $\\nabla_\\mathbf{x} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}) = \\mathbf{0}$. This will give us three equations:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial x_1} & = 0\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial x_2} & = 0\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial x_3} & = 0 .\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We also have a fourth equation corresponding to the equality constraint:\n",
    "$$\n",
    "h(\\mathbf{x}) = 2x_1 - x_2 + x_3 - 3 = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H6kT37PR6Nic"
   },
   "source": [
    "### Exercise\n",
    "\n",
    "On paper, work out the three partial derivatives above, corresponding to $\\nabla_\\mathbf{x} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}) = \\mathbf{0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h0eQEBTp6coR"
   },
   "source": [
    "So to recap, we have a system of four equations and four variables, and we simply need to find the root (i.e. the solution for which each of those equations equal zero). Now, of course, you could work this out by hand. But we're interest in seeing how Python can help us out.\n",
    "\n",
    "Let's use `autograd` from Unit 3 to compute the gradients. When you worked out these by hand, you would have seen that they are pretty easy partial derivatives. But `autograd` is nice when we have more complicated expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fRu5Bwwr7XJJ"
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G4B9oe6s7g-B"
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "  \"Our original objective function\"\n",
    "  x1, x2, x3 = x\n",
    "  return x1**2 + x2**2 + x3**2\n",
    "\n",
    "def h(x):\n",
    "  \"Our equality constraint\"\n",
    "  x1, x2, x3 = x\n",
    "  return 2*x1 - x2 + x3 - 3\n",
    "\n",
    "def lagrangian(l):\n",
    "  \"The Lagrangian function\"\n",
    "  x1, x2, x3, _lambda = l  # note that lambda is a reserved word in Python\n",
    "  return f([x1, x2, x3]) + _lambda * h([x1, x2, x3])\n",
    "\n",
    "# Gradients of the Lagrangian\n",
    "dLdl = grad(lagrangian)  # courtesy of autograd, nice!\n",
    "\n",
    "def obj(l):\n",
    "  x1, x2, x3, _lambda = l\n",
    "  dLdx1, dLdx2, dLdx3, dLdlam = dLdl(l)\n",
    "  # return the terms that should be set to zero (note we throw away dLdlam)\n",
    "  return [dLdx1, dLdx2, dLdx3, h([x1, x2, x3])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U2pMmeXt9kMB"
   },
   "source": [
    "So we now have an expression the returns each of the equations that must equal\n",
    "zero. The last step is to solve this thing.\n",
    "\n",
    "`scipy.optimize` provides a generic tool for root finding called `fsolve`. It has a similar interface to `minimize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VPZuU2Ek991o"
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import fsolve\n",
    "l0 = np.array([0.0, 0.0, 0.0, 1.0])  # the initial values of x1, x2, x3, _lambda\n",
    "x1, x2, x3, _lambda = fsolve(obj, l0)  # note this is the variable 'l0' not 10\n",
    "print(f'The solution is at {x1, x2, x3} and lambda={_lambda}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZrzXS4YIC4Dh"
   },
   "source": [
    "### Exercise\n",
    "\n",
    "It was kind of wasteful to compute $\\frac{\\partial \\mathcal{L}}{\\partial \\lambda}$ and then throw it away. Can you rewrite the code to only compute the necessary partial derivatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1QDSo6IoDOfY"
   },
   "source": [
    "### Exercise\n",
    "\n",
    "What happens if we formulate the Lagrangian as $\n",
    "\\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}) = f(\\mathbf{x}) - \\lambda h(\\mathbf{x})$ (i.e. we subtract rather than add the penalty term)? You may wish to check out [this Stack Exchange post](https://math.stackexchange.com/q/1099429) for a discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QqtUdqBdGNut"
   },
   "source": [
    "Now, how do we know whether the function is at a minimum? We can check that the Hessian is positive definite in the original objective $f(\\mathbf{x})$. `autograd` makes this really easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FQL94aS7GiPU"
   },
   "outputs": [],
   "source": [
    "from autograd import hessian\n",
    "hess = hessian(f)\n",
    "hess(np.array([x1, x2, x3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WmVdZHXXG1Ze"
   },
   "source": [
    "A positive definite Hessian will have all positive eigenvalues. The eigenvalues should be obvious from the diagonal structure of the Hessian. But we can check this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w_os-bTGHC-V"
   },
   "outputs": [],
   "source": [
    "eigvals, eigvecs = np.linalg.eig(hess(np.array([x1, x2, x3])))\n",
    "print(eigvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZCRu1Lo3Nh2X"
   },
   "source": [
    "## Constrained minimization of multivariate scalar functions (`minimize`)\n",
    "\n",
    "The `minimize` function provides algorithms for **constrained** minimization, namely `trust-constr`, `SLSQP` and `COBYLA`. They require the constraints to be defined using slightly different structures. The method `trust-constr` requires the constraints to be defined as a sequence of objects [LinearConstraint](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.LinearConstraint.html#scipy.optimize.LinearConstraint) or [NonlinearConstraint](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.NonlinearConstraint.html#scipy.optimize.NonlinearConstraint). Methods `SLSQP` and `COBYLA`, on the other hand, require constraints to be defined as a sequence of dictionaries, with keys `type`, `fun`, and `jac`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v61sEi1GNh2X"
   },
   "source": [
    "## Trust-Region Constrained Algorithm (`method='SLSQP'`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kw_r0lT2Nh2j"
   },
   "source": [
    "We've already formulated the objective and constraints, so let's see how easy it is to use the method of Sequential Least SQuares Programming (SLSQP) provided by `scipy.optimize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "RL0DpN7-FKhI"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "from scipy.optimize import minimize\n",
    "sol = minimize(f, [1, -0.5, 0.5], constraints={'type': 'eq', 'fun': h},\n",
    "               options={'disp': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hM0c5SIfFTBA"
   },
   "source": [
    "We note that the solution is the same as above. That was a lot easier than forming the Lagrangian ourselves!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qD2yxMZcFsyE"
   },
   "source": [
    "### Exercise\n",
    "\n",
    "The `scipy.optimize` default solver for constrained problems is SQSLP.  Try solving this problem using the other constrained minimization algorithms available through `minimize`: `trust-constr` and `COBYLA`. You will need to specify a `method` argument."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ConstrainedOptimization.ipynb",
   "provenance": [
    {
     "file_id": "1FMM-aAALAeLY-ctwPOmjeadN2eFWjsHC",
     "timestamp": 1569985457190
    },
    {
     "file_id": "1u9v8Xc7KilebO7Wti1ysIJWQmWNi8VS2",
     "timestamp": 1569981925535
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
